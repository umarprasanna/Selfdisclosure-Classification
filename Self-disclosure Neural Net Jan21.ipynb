{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "from itertools import groupby\n",
    "import contractions\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import chardet \n",
    "from os.path import join, exists, split\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from copy import deepcopy\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Functions for text pre-processing\n",
    "\n",
    "def unescapematch(matchobj):\n",
    "    \"\"\"\n",
    "    Converts from hex to unicode: \\u201c -> '\n",
    "    \"\"\"\n",
    "    escapesequence = matchobj.group(0)\n",
    "    digits = escapesequence[2:6]\n",
    "    ordinal = int(digits, 16)\n",
    "    char = chr(ordinal)\n",
    "    return char\n",
    "\n",
    "def replace_url_phone(text):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) emails with emailid\n",
    "    2) urls with url\n",
    "    3) phone numbers with phonenumber\n",
    "    \"\"\"\n",
    "    email_regex = (\"([A-Za-z0-9!#$%&'*+\\/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+\\/=?^_`\"\"{|}~-]+)*(@)(?:[A-Za-z0-9](?:[a-z0-9-]*[a-z0-9])?(\\.|\"\"\\sdot\\s))+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?)\")\n",
    "    url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    phone_regex =  (\"([+]\\d{12}|[+]?\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)[-\\.\\s]??\\d{3}[\\-\\.\\s]??\\d{4}|[+]\\d{1,2}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4})\")\n",
    "    text = re.sub(email_regex, 'emailid',text)\n",
    "    text = re.sub(url_regex, 'url',text)\n",
    "    text = re.sub(phone_regex, 'phonenumber', text)\n",
    "    return text\n",
    " \n",
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    Cleans string\n",
    "    1) replaces e.g: \\u201c -> '\n",
    "    2) replaces contractions lile I'm -> I am\n",
    "    3) replaces emailids, urls and phone numbers\n",
    "    4) places a space between words and punctuation\n",
    "    5) replaces symbols with words like $ -> dollar\n",
    "    \n",
    "    \"\"\"\n",
    "    string = re.sub(r'(\\\\u[0-9A-Fa-f]{4})', unescapematch, string)\n",
    "    # remove remaining hexcodes\n",
    "    string = re.sub(r'[^\\x00-\\x7f]',r'', string)\n",
    "    string = contractions.fix(string)\n",
    "    string = replace_url_phone(string)\n",
    "    # add space between punctuation and text\n",
    "    string = re.sub(r\"([\\w/'+$\\s-]+|[^\\w/'+$\\s-]+)\\s*\", r\"\\1 \", string)\n",
    "    string = re.sub('\\$', \" dollar \", string)\n",
    "    string = re.sub('\\%', \" percent \", string)\n",
    "    string = re.sub('\\&', \" and \", string)\n",
    "    string = re.sub('\\\"',\" quote \", string)\n",
    "    string = string.replace(\"\\\\\",\"\")\n",
    "    # remove multiple instances of punctuation\n",
    "    re.sub(r'(\\W)(?=\\1)', '', string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "def check_encoding(filepath): \n",
    "    \"\"\"\n",
    "    Check encoding of a file \n",
    "    \"\"\"\n",
    "    rawdata = open(filepath, 'rb').read()\n",
    "    result = chardet.detect(rawdata)\n",
    "    charenc = result['encoding']\n",
    "    return charenc\n",
    "\n",
    "def replace_entity(sentence):\n",
    "    \"\"\"\n",
    "    Replaces specific entities in text\n",
    "    Ex: Harry lives in Pennsylvania -> PERSON lives in GPE \n",
    "    \n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    sentence_ = []\n",
    "    for ent in doc.ents:\n",
    "        ent.merge()\n",
    "    for token in doc:\n",
    "        \n",
    "        if token.ent_type_ in ['ORG','GPE','LOC','NORP','CARDINAL','FACILITY','MONEY','PERSON','DATE','TIME',\\\n",
    "                      'PERCENT','QUANTITY','ORDINAL']:\n",
    "            sentence_.append(token.ent_type_+'_ent')\n",
    "            \n",
    "        else:\n",
    "            sentence_.append(token.orth_)\n",
    "        # combining detection of compound entities like State-College (ORG ORG ORG) -> ORG\n",
    "        sentence_set = [x[0] for x in groupby(sentence_)]\n",
    "    return sentence_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and prepare for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Functions to load data and produce embeddings\n",
    "def load_data_and_labels(folderpath,filename):\n",
    "    \"\"\"\n",
    "    Loads data, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    with open(os.path.join(folderpath, filename), 'r',encoding=\"utf-8\") as datafile:\n",
    "         reader = csv.reader(datafile,delimiter='\\t')\n",
    "         first_iteration = 1\n",
    "         x_text = []\n",
    "         y = []\n",
    "         for row in reader:\n",
    "             if first_iteration==1:\n",
    "                 print('The columns in the file are:\\n ',row)\n",
    "                 #text_index = row.index('CommentText')\n",
    "                 text_index = row.index('SENTENCES')\n",
    "                 class_index = row.index('binary_labels')\n",
    "                 first_iteration =0\n",
    "             else:\n",
    "                #text = clean_string(row[text_index])\n",
    "                comment = eval(row[text_index])\n",
    "                labels = eval(row[class_index])\n",
    "                if len(comment)>0:\n",
    "                   for i in range(len(comment)):\n",
    "                       sentence = clean_string(comment[i])\n",
    "                       sentence = replace_entity(sentence)\n",
    "                       sentence = [word.lower() for word in sentence]\n",
    "                       label = labels[i][0]\n",
    "                       x_text.append(sentence)\n",
    "                       y.append(label)\n",
    "         return x_text,y\n",
    "         \n",
    "        \n",
    "def pad_sequences(sentences, pad_len, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads the sentences to make the length equal to the sentence with maximum number of words\n",
    "    \"\"\"\n",
    "    if pad_len is not None:\n",
    "        sequence_length = pad_len\n",
    "    else:\n",
    "        sequence_length = max(len(x) for x in sentences)\n",
    "    \n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        if len(sentence)<sequence_length:\n",
    "           num_padding = sequence_length - len(sentence)\n",
    "           new_sentence = sentence + [padding_word] * num_padding\n",
    "        else:\n",
    "           num_padding = 0\n",
    "           new_sentence = [x for x in sentence[0:pad_len]]\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv\n",
    "\n",
    "def load_data(folderpath,filename):\n",
    "    \"\"\"\n",
    "    Loads the preprocessed data.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences, labels = load_data_and_labels(folderpath,filename)\n",
    "    sentences_padded = pad_sequences(sentences, padding_word=\"<PAD/>\", pad_len=30)\n",
    "    vocab_size,vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    return [x, y, vocabulary, vocabulary_inv, vocab_size] \n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "def load_trainable_dataset(folderpath,filename):\n",
    "    \n",
    "    x_text, y, vocabulary, vocabulary_inv_list, vocab_size = load_data(folderpath,filename)\n",
    "    \n",
    "    vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "    \n",
    "    #y = y.argmax(axis=1)\n",
    "\n",
    "    # Shuffle data\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    \n",
    "    x_text = x_text[shuffle_indices]\n",
    "    y = y[shuffle_indices]\n",
    "    \n",
    "    train_len = int(len(x_text) * 0.8)\n",
    "    \n",
    "    x_train = x_text[:train_len]\n",
    "    y_train = y[:train_len]\n",
    "    x_test = x_text[train_len:]\n",
    "    y_test = y[train_len:]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, vocabulary, vocabulary_inv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Keras F1, precision and recall metric with callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Custom Keras metric\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from keras.callbacks import Callback\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "         self.val_f1s = []\n",
    "         self.val_recalls = []\n",
    "         self.val_precisions = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "         val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "         val_targ = self.validation_data[1]\n",
    "         _val_f1 = f1_score(val_targ, val_predict)\n",
    "         _val_recall = recall_score(val_targ, val_predict)\n",
    "         _val_precision = precision_score(val_targ, val_predict)\n",
    "         self.val_f1s.append(_val_f1)\n",
    "         self.val_recalls.append(_val_recall)\n",
    "         self.val_precisions.append(_val_precision)\n",
    "         print('— val_f1: %f — val_precision: %f — val_recall %f' %(_val_f1, _val_precision, _val_recall))\n",
    "         return \n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating embeddings: Google news trained and trained from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and pretrained word2vec model....\n",
      "The columns in the file are:\n",
      "  ['CommentID', 'ArticleId', 'NewsSource', 'NewsTitle', 'ParentId', 'CommentID', 'CommentText', 'TimeStampId', 'Commentor', 'CommentorInfo', 'SENTENCES', 'SVO', 'FACILITY', 'LOC', 'GPE', 'NORP', 'ORG', 'Category', 'Category_sentence', 'binary_labels']\n",
      "Padding sentences..............\n",
      "Online training the google pretrained word2vec model\n",
      "Re-trained word2vec model\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "\n",
    "print('Loading data and pretrained word2vec model....')\n",
    "sentences,labels = load_data_and_labels('/Users/pxu3/Desktop/Spring 2019/Research/Data/','Binarized_Dataset.txt')\n",
    "google_wv = KeyedVectors.load_word2vec_format('/Users/pxu3/Desktop/Deep Learning/1Bword2vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "google_wv.save(\"/Users/pxu3/Desktop/Spring 2019/Research/Code/pretrained_googlewv\")\n",
    "\n",
    "print('Padding sentences..............')\n",
    "padded_sentences = pad_sequences(sentences, padding_word=\"<PAD/>\", pad_len=30)\n",
    "\n",
    "print('Online training the google pretrained word2vec model')\n",
    "model = Word2Vec(size=300, min_count=1, iter=10)\n",
    "model.build_vocab(padded_sentences)\n",
    "training_examples_count = model.corpus_count\n",
    "# below line will make it 1, so saving it before\n",
    "model.build_vocab([list(google_wv.vocab.keys())], update=True)\n",
    "model.intersect_word2vec_format(\"/Users/pxu3/Desktop/Deep Learning/1Bword2vec/GoogleNews-vectors-negative300.bin\",binary=True, lockf=1.0)\n",
    "model.train(padded_sentences,total_examples=training_examples_count, epochs=model.epochs)\n",
    "model.save(\"/Users/pxu3/Desktop/Spring 2019/Research/Code/updated_pretrained_googlewv.bin\")\n",
    "\n",
    "print('Re-trained word2vec model')\n",
    "model1 = Word2Vec(size=300, min_count=1, iter=10)\n",
    "model1.build_vocab(padded_sentences)\n",
    "model1.train(padded_sentences,total_examples=training_examples_count, epochs=model.epochs)\n",
    "model1.save(\"/Users/pxu3/Desktop/Spring 2019/Research/Code/trained_wv.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gpe_ent', 0.9062396883964539), ('isis', 0.833358883857727), ('land', 0.8105503916740417), ('org_ent', 0.7978490591049194), ('military', 0.7964844703674316), ('water', 0.7870767116546631), ('countries', 0.7859431505203247), ('west', 0.7794488668441772), ('country', 0.7779181003570557), ('world', 0.7766268253326416)] [('gpe_ent', 0.8147958517074585), ('region', 0.7576183080673218), ('west', 0.752234935760498), ('south', 0.7443078756332397), ('border', 0.7019321322441101), ('waters', 0.7007555961608887), ('reefs', 0.6848381757736206), ('north', 0.6750556826591492), ('land', 0.6741229891777039), ('drought', 0.6426279544830322)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('loc_ent'),model1.wv.most_similar('loc_ent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns in the file are:\n",
      "  ['CommentID', 'ArticleId', 'NewsSource', 'NewsTitle', 'ParentId', 'CommentID', 'CommentText', 'TimeStampId', 'Commentor', 'CommentorInfo', 'SENTENCES', 'SVO', 'FACILITY', 'LOC', 'GPE', 'NORP', 'ORG', 'Category', 'Category_sentence', 'binary_labels']\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, vocabulary, vocabulary_inv = load_trainable_dataset('/Users/pxu3/Desktop/Spring 2019/Research/Data/','Binarized_Dataset.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "#tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "#tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "#tf.flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding (default: 300)\")\n",
    "#tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "#tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "#tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "#tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "#tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "#tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "#tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "#tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "#tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "#tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "#tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "#FLAGS = tf.flags.FLAGS\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, model_path, sequence_length, num_classes,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.model_path = model_path\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        \n",
    "        # Load the word-vector model\n",
    "        \n",
    "        model = Word2Vec.load(self.model_path)\n",
    "        wvecs = model.wv\n",
    "        embedding_size = wvecs.vector_size\n",
    "        vocab_size = len(wvecs.vocab)\n",
    "\n",
    "        # Create the embedding matrix where words are indexed alphabetically\n",
    "        embedding_mat = np.zeros(shape=(vocab_size, embedding_size), dtype='int32')\n",
    "        for idx, word in enumerate(sorted(wvecs.vocab)):\n",
    "            embedding_mat[idx] = wvecs.get_vector(word)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            #self.W = tf.get_variable(\"embedding\", [vocab_size, embedding_size],\\\n",
    "            #                               initializer=tf.constant_initializer(embedding_mat),trainable=False)\n",
    "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b10a083ecf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 l2_reg_lambda=FLAGS.l2_reg_lambda) \n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-99c152256954>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, sequence_length, num_classes, embedding_size, filter_sizes, num_filters, l2_reg_lambda)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0membedding_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0membedding_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Embedding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,\\\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=x_train.shape[1],\n",
    "                num_classes=2,\n",
    "                model_path = \"/Users/pxu3/Desktop/Spring 2019/Research/Code/updated_pretrained_googlewv.bin\",\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
